{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We continue to consider the use of a logistic regression model to\n",
    "predict the probability of default using income and balance on the\n",
    "Default data set. In particular, we will now compute estimates for\n",
    "the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using\n",
    "the standard formula for computing the standard errors in the glm()\n",
    "function. Do not forget to set a random seed before beginning your\n",
    "analysis\n",
    "\n",
    "(a) Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income\n",
       "1      No      No   729.526495  44361.625074\n",
       "2      No     Yes   817.180407  12106.134700\n",
       "3      No      No  1073.549164  31767.138947\n",
       "4      No      No   529.250605  35704.493935\n",
       "5      No      No   785.655883  38463.495879"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default = pd.read_csv(\"Data-Default.csv\", index_col=0)\n",
    "print(default.shape)\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default student      balance        income\n",
       "1        0      No   729.526495  44361.625074\n",
       "2        0     Yes   817.180407  12106.134700\n",
       "3        0      No  1073.549164  31767.138947\n",
       "4        0      No   529.250605  35704.493935\n",
       "5        0      No   785.655883  38463.495879"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_dict = {\"Yes\":1, \"No\":0}\n",
    "default[\"default\"] = default[\"default\"].map(encoding_dict)\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   const      balance        income\n",
       "1    1.0   729.526495  44361.625074\n",
       "2    1.0   817.180407  12106.134700\n",
       "3    1.0  1073.549164  31767.138947\n",
       "4    1.0   529.250605  35704.493935\n",
       "5    1.0   785.655883  38463.495879"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = default[[\"balance\", \"income\"]]\n",
    "X = sm.add_constant(X)\n",
    "y = default[\"default\"]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Thu, 25 Feb 2021   Pseudo R-squ.:                  0.4594\n",
      "Time:                        23:36:06   Log-Likelihood:                -789.48\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "results = sm.Logit(y,X).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5615709310356376"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.exp(0.0056)-1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002081021652955428"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.exp(2.081*10**-5)-1)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of balance is 0.5616 and the standard error of income is 0.00208."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Write a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(data, num_samples):\n",
    "    positive_data = data[data[\"default\"]==1]\n",
    "    negative_data = data[data[\"default\"]==0]\n",
    "    \n",
    "    positive_indices = np.random.choice(positive_data.index, int(num_samples/4), replace=True)\n",
    "    negative_indices = np.random.choice(negative_data.index, int(3*num_samples/4), replace=True)\n",
    "    total = np.concatenate([positive_indices, negative_indices])\n",
    "    np.random.shuffle(total)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot(data, func, R):\n",
    "    total_coeff_balance = []\n",
    "    total_coeff_income = []\n",
    "    for i in range(R):\n",
    "        bootstrap = resample(data, replace=True, n_samples=(0.3*default.size), random_state=1, stratify = data['default'])\n",
    "        params = func(data, bootstrap.index)\n",
    "        total_coeff_balance.append(params[0])\n",
    "        total_coeff_income.append(params[1])\n",
    "    return (np.mean(total_coeff_balance), np.mean(total_coeff_income))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_fn(data, index):\n",
    "    X = data[[\"balance\", \"income\"]].loc[index]\n",
    "    y = data[\"default\"].loc[index]\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    intercept = lr.intercept_\n",
    "    coef_balance = lr.coef_[0][0]\n",
    "    coef_income = lr.coef_[0][1]\n",
    "    return [intercept,coef_balance, coef_income]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Use the boot() function together with your boot.fn() function to\n",
    "estimate the standard errors of the logistic regression coefficients\n",
    "for income and balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept is [-7.42836654e-09], the coeff of balance is 0.00230419241395492, and the coeff of income is -0.00010973755734612088\n"
     ]
    }
   ],
   "source": [
    "intercept, coef_balance, coef_income = boot_fn(default, get_indices(default, 100))\n",
    "\n",
    "print(f'Intercept is {intercept}, the coeff of balance is {coef_balance}, and the coeff of income is {coef_income}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Comment on the estimated standard errors obtained using the\n",
    "glm() function and using your bootstrap function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are similar, but not exactly the same. However, the fact that they're as similar as they are demonstrates how powerful bootstrapping can be in estimating variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. We will now perform cross-validation on a simulated data set.\n",
    "(a) Generate a simulated data set as follows:\n",
    "> set.seed(1)\n",
    "> x=rnorm(100)\n",
    "> y=x-2*x^2+rnorm (100)\n",
    "\n",
    "In this data set, what is n and what is p? Write out the model\n",
    "used to generate the data in equation form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "X = np.random.normal(size = 100)\n",
    "y = X-2*(X ** 2) + np.random.normal(size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 (b) Create a scatterplot of X against Y . Comment on what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Scatter Plot')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(X,y)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Scatter Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot appears to show a quadratic function affected by some random noise. This is not surprising, as the function we input into y was simply a quadratic function of X, with some random noise added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 (c) Set a random seed, and then compute the LOOCV errors that\n",
    "result from fitting the following four models using least squares:\n",
    "$$i. Y = \\beta0 + \\beta1X +\\epsilon$$\n",
    "$$ii. Y = \\beta0 + \\beta1X + \\beta2X^2 +\\epsilon$$\n",
    "$$iii. Y = \\beta0 + \\beta1X + \\beta2X^2 + \\beta3X^3 +\\epsilon$$\n",
    "$$iv. Y = \\beta0 + \\beta1X + \\beta2X^2 + \\beta3X^3 + \\beta4X^4 + \\epsilon$$\n",
    "Note you may find it helpful to use the data.frame() function\n",
    "to create a single data set containing both X and Y ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model1, error is 6.260764331604616\n",
      "For model2, error is 0.9142897072803657\n",
      "For model3, error is 0.9268768781648801\n",
      "For model4, error is 0.8669116865881077\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "for i in range(1,5):\n",
    "    poly = PolynomialFeatures(i, include_bias=False)\n",
    "    predictors = poly.fit_transform(X.reshape(-1,1))\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    error = -1*cross_val_score(lr, predictors, y, cv=len(X), scoring =\"neg_mean_squared_error\").mean()\n",
    "    print(f'For model{i}, error is {error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Repeat (c) using another random seed, and report your results.\n",
    "Are your results the same as what you got in (c)? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model1, error is 6.260764331604616\n",
      "For model2, error is 0.9142897072803657\n",
      "For model3, error is 0.9268768781648801\n",
      "For model4, error is 0.8669116865881077\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "for i in range(1,5):\n",
    "    poly = PolynomialFeatures(i, include_bias=False)\n",
    "    predictors = poly.fit_transform(X.reshape(-1,1))\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    error = -1*cross_val_score(lr, predictors, y, cv=len(X), scoring =\"neg_mean_squared_error\").mean()\n",
    "    print(f'For model{i}, error is {error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOOCV errors are identical. This makes sense because the random seed doesn't change the underlying math of LOOCV; there is no random sampling. LOOCV is set up to iterate through the entire data set, leaving out each observation out of the training set exactly once. Because every observation is left out exactly once regardless of random seed, altering the seed does not change the LOOCV error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Which of the models in (c) had the smallest LOOCV error? Is\n",
    "this what you expected? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest LOOCV error is in the 2nd degree polynomial estimate. This makes sense because our underlying function is 2nd degree. Even if we didn't know the underlying function, visual inspection of the scatterplot is highly suggestive of a quadratic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.093\n",
      "Model:                            OLS   Adj. R-squared:                  0.083\n",
      "Method:                 Least Squares   F-statistic:                     9.997\n",
      "Date:                Thu, 25 Feb 2021   Prob (F-statistic):            0.00209\n",
      "Time:                        23:36:33   Log-Likelihood:                -228.87\n",
      "No. Observations:                 100   AIC:                             461.7\n",
      "Df Residuals:                      98   BIC:                             466.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.4131      0.242     -5.849      0.000      -1.893      -0.934\n",
      "x1             0.8610      0.272      3.162      0.002       0.321       1.401\n",
      "==============================================================================\n",
      "Omnibus:                       37.310   Durbin-Watson:                   1.661\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               69.521\n",
      "Skew:                          -1.554   Prob(JB):                     8.01e-16\n",
      "Kurtosis:                       5.651   Cond. No.                         1.15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.863\n",
      "Model:                            OLS   Adj. R-squared:                  0.860\n",
      "Method:                 Least Squares   F-statistic:                     304.9\n",
      "Date:                Thu, 25 Feb 2021   Prob (F-statistic):           1.47e-42\n",
      "Time:                        23:36:33   Log-Likelihood:                -134.42\n",
      "No. Observations:                 100   AIC:                             274.8\n",
      "Df Residuals:                      97   BIC:                             282.7\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1350      0.115      1.169      0.245      -0.094       0.364\n",
      "x1             1.0936      0.107     10.229      0.000       0.881       1.306\n",
      "x2            -1.9846      0.085    -23.331      0.000      -2.153      -1.816\n",
      "==============================================================================\n",
      "Omnibus:                        0.893   Durbin-Watson:                   2.152\n",
      "Prob(Omnibus):                  0.640   Jarque-Bera (JB):                0.552\n",
      "Skew:                          -0.170   Prob(JB):                        0.759\n",
      "Kurtosis:                       3.132   Cond. No.                         2.10\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.865\n",
      "Model:                            OLS   Adj. R-squared:                  0.861\n",
      "Method:                 Least Squares   F-statistic:                     204.8\n",
      "Date:                Thu, 25 Feb 2021   Prob (F-statistic):           1.40e-41\n",
      "Time:                        23:36:33   Log-Likelihood:                -133.66\n",
      "No. Observations:                 100   AIC:                             275.3\n",
      "Df Residuals:                      96   BIC:                             285.7\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1280      0.115      1.111      0.269      -0.101       0.357\n",
      "x1             0.9065      0.187      4.842      0.000       0.535       1.278\n",
      "x2            -1.9753      0.085    -23.187      0.000      -2.144      -1.806\n",
      "x3             0.0788      0.065      1.216      0.227      -0.050       0.208\n",
      "==============================================================================\n",
      "Omnibus:                        1.539   Durbin-Watson:                   2.129\n",
      "Prob(Omnibus):                  0.463   Jarque-Bera (JB):                1.081\n",
      "Skew:                          -0.236   Prob(JB):                        0.583\n",
      "Kurtosis:                       3.193   Cond. No.                         5.53\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.873\n",
      "Model:                            OLS   Adj. R-squared:                  0.867\n",
      "Method:                 Least Squares   F-statistic:                     163.0\n",
      "Date:                Thu, 25 Feb 2021   Prob (F-statistic):           1.24e-41\n",
      "Time:                        23:36:33   Log-Likelihood:                -130.63\n",
      "No. Observations:                 100   AIC:                             271.3\n",
      "Df Residuals:                      95   BIC:                             284.3\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.3140      0.136      2.311      0.023       0.044       0.584\n",
      "x1             0.9127      0.183      4.999      0.000       0.550       1.275\n",
      "x2            -2.5445      0.248    -10.264      0.000      -3.037      -2.052\n",
      "x3             0.0992      0.064      1.556      0.123      -0.027       0.226\n",
      "x4             0.1394      0.057      2.437      0.017       0.026       0.253\n",
      "==============================================================================\n",
      "Omnibus:                        1.537   Durbin-Watson:                   2.100\n",
      "Prob(Omnibus):                  0.464   Jarque-Bera (JB):                1.088\n",
      "Skew:                          -0.238   Prob(JB):                        0.581\n",
      "Kurtosis:                       3.184   Cond. No.                         15.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    poly = PolynomialFeatures(i)\n",
    "    predictors = poly.fit_transform(X.reshape(-1,1))\n",
    "    \n",
    "    results = sm.OLS(y, predictors).fit()\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of the models, X is significant at the p<0.05 level. $X^2$ is also significant at the p<0.05 level in models 2-5. However, $X^3$ and $X^4$ are not significant at the p<0.05 level in any of the models they are in.\n",
    "\n",
    "This is consistent with the cross-validation results, in that the LOOCV showed that model 2, which included only X and $X^2$, had the lowest error. As we add higher level predictors, we are increasing the variablity/noise of our estimates, but not necessarily gaining any additional accuracy when predicting the test observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. We will now try to predict per capita crime rate in the Boston data\n",
    "set.\n",
    "(a) Try out some of the regression methods explored in this chapter,\n",
    "such as best subset selection, the lasso, ridge regression, and\n",
    "PCR. Present and discuss results for the approaches that you\n",
    "consider.\n",
    "\n",
    "Use best subset, forward stepwise, & backwards stepwise selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO       B  \\\n",
       "0  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0     15.3  396.90   \n",
       "1   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0     17.8  396.90   \n",
       "2   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0     17.8  392.83   \n",
       "3   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0     18.7  394.63   \n",
       "4   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0     18.7  396.90   \n",
       "\n",
       "   LSTAT  \n",
       "0   4.98  \n",
       "1   9.14  \n",
       "2   4.03  \n",
       "3   2.94  \n",
       "4   5.33  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = data.drop(\"CRIM\", axis=1)\n",
    "y=data[\"CRIM\"]\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Subset - Predictors that make the best model are:  ('ZN', 'NOX', 'DIS', 'RAD', 'LSTAT') with a score of  -44.048606471148375\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "data = pd.DataFrame(boston.data,columns = boston.feature_names)\n",
    "X = data.drop('CRIM',axis = 1)\n",
    "y = data['CRIM']\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "### Best Subset ###\n",
    "\n",
    "P = len(X.columns)\n",
    "used_pred = []\n",
    "M = []\n",
    "M_scores = []\n",
    "shrinking_subset = list(X.columns)\n",
    "predictors = []\n",
    "\n",
    "for K in range(1, P+1):\n",
    "    #predictors = []\n",
    "    for combination in it.combinations(X.columns, K):\n",
    "        #print(\"combination = \", combination,\"\\n\")\n",
    "        predictors.append(combination)\n",
    "        #print(\"predictors = \", predictors,\"\\n\")\n",
    "\n",
    "combo_set = set(predictors)\n",
    "for combo in combo_set:\n",
    "    combo\n",
    "    score = np.mean(cross_val_score(lin_reg, X[list(combo)], y, cv = 5, scoring = 'neg_mean_squared_error'))\n",
    "    #print(\"score = \", score, \"\\n\")\n",
    "    M.append(combo)\n",
    "    M_scores.append(score)                             \n",
    "best_M = M_scores.index(max(M_scores))\n",
    "print('Best Subset - Predictors that make the best model are: ', M[best_M], 'with a score of ', max(M_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Subset - Predictors that make the best model are:  ('ZN', 'NOX', 'DIS', 'RAD', 'LSTAT') with a score of  -44.048606471148375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection - Predictors that make the best model are:  ['RAD', 'LSTAT', 'ZN'] with a score of  -44.10339547727301\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "data = pd.DataFrame(boston.data,columns = boston.feature_names)\n",
    "X = data.drop('CRIM',axis = 1)\n",
    "y = data['CRIM']\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "### Forward Selection ###\n",
    "\n",
    "P = len(X.columns)\n",
    "used_pred = []\n",
    "M = []\n",
    "M_scores = []\n",
    "\n",
    "for K in range(P):\n",
    "    best_score = -1000\n",
    "    best_pred = None\n",
    "    #print (\"K = \", K, \"\\n\")\n",
    "    # Inner loop\n",
    "    for var in X.columns:\n",
    "        #print(\"var = \", var, \"\\n\")\n",
    "        #print(\"Used = \", used_pred, \"\\n\")\n",
    "        # Skips if predictor already used\n",
    "        if var not in used_pred:\n",
    "            predictors = used_pred[:]   \n",
    "            predictors.append(var)\n",
    "            \n",
    "            score = np.mean(cross_val_score(lin_reg, X[predictors], y, cv = 5, scoring = 'neg_mean_squared_error'))\n",
    "            #print(\"score = \", score, \"\\n\")\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred = var\n",
    "    \n",
    "    # Updates the list of used predictors and list of Mk models\n",
    "    used_pred.append(best_pred)\n",
    "    M.append(used_pred[:])\n",
    "    M_scores.append(best_score)                             \n",
    "    \n",
    "best_M = M_scores.index(max(M_scores))\n",
    "print('Forward Selection - Predictors that make the best model are: ', M[best_M], 'with a score of ', max(M_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Selection - Predictors that make the best model are:  ['ZN', 'NOX', 'DIS', 'RAD', 'LSTAT'] with a score of  -44.048606471148375\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "data = pd.DataFrame(boston.data,columns = boston.feature_names)\n",
    "X = data.drop('CRIM',axis = 1)\n",
    "y = data['CRIM']\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "### Backward Selection ###\n",
    "\n",
    "P = len(X.columns)\n",
    "M = list(X.columns)\n",
    "\n",
    "M_scores = [[np.mean(cross_val_score(lin_reg, X[list.copy(M)], y, cv = 5, scoring = 'neg_mean_squared_error'))]]\n",
    "best_models = [M]\n",
    "for K in range(P-1):\n",
    "\n",
    "    best_score = -1000\n",
    "    best_pred = None\n",
    "        \n",
    "    # Inner loop\n",
    "    for var in M:\n",
    "        predictors = list.copy(M)   \n",
    "        predictors.remove(var)\n",
    "\n",
    "        score = np.mean(cross_val_score(lin_reg, X[predictors], y, cv = 5, scoring = 'neg_mean_squared_error'))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_pred = predictors\n",
    " \n",
    "    M = list.copy(best_pred)\n",
    "    \n",
    "    best_models.append(best_pred)\n",
    "    M_scores.append(best_score)  \n",
    "     \n",
    "best_M = M_scores.index(max(M_scores))\n",
    "print('Backward Selection - Predictors that make the best model are: ', best_models[best_M], \"with a score of \", max(M_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error.\n",
    "\n",
    "Compare the results of using the mathematical-adjustment approaches(Cp,AIC,BIC, & adjustedR2) to using 5-Fold Cross-Validation (5FCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['CRIM']\n",
    "\n",
    "# The predictors from forwards validation\n",
    "X_1 = data[['RAD', 'LSTAT', 'ZN']]\n",
    "# The predictors from Best Subset and Backwards validation\n",
    "X_2 = data[['ZN', 'NOX', 'DIS', 'RAD', 'LSTAT']]\n",
    "\n",
    "# All predictors\n",
    "X_all = data.drop('CRIM', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL 1\n",
      "5FCV =  44.10339547727301\n",
      "Adjusted R^2 =  0.41994989563385443\n",
      "AIC =  3342.1268133254403\n",
      "BIC =  3359.0329600025902\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL 1\")\n",
    "\n",
    "\n",
    "lm_1 = smf.ols(\"CRIM ~ RAD + LSTAT + ZN\" , data = data).fit()\n",
    "\n",
    "print(\"5FCV = \", -1*cross_val_score(lr, X_1, y, cv=5, scoring=\"neg_mean_squared_error\").mean())\n",
    "print(\"Adjusted R^2 = \", lm_1.rsquared_adj)\n",
    "print(\"AIC = \", lm_1.aic)\n",
    "print(\"BIC = \", lm_1.bic)\n",
    "\n",
    "#lr.LassoLarsIC(criterion = \"aic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL 2\n",
      "5FCV =  44.048606471148375\n",
      "Adjusted R^2 =  0.425035469531877\n",
      "AIC =  3339.650946812067\n",
      "BIC =  3365.010166827792\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL 2\")\n",
    "lm_2 = smf.ols(\"CRIM ~ RAD + LSTAT + ZN + NOX + DIS\" , data = data).fit()\n",
    "\n",
    "print(\"5FCV = \", -1*cross_val_score(lr, X_2, y, cv=5, scoring=\"neg_mean_squared_error\").mean())\n",
    "print(\"Adjusted R^2 = \", lm_2.rsquared_adj)\n",
    "print(\"AIC = \", lm_2.aic)\n",
    "print(\"BIC = \", lm_2.bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE EVERYTHING MODEL\n",
      "5FCV =  48.57680751139641\n",
      "Adjusted R^2 =  0.425035469531877\n",
      "AIC =  3339.650946812067\n",
      "BIC =  3365.010166827792\n"
     ]
    }
   ],
   "source": [
    "print(\"THE EVERYTHING MODEL\")\n",
    "\n",
    "lm_all = smf.ols(\"CRIM ~ RAD + LSTAT + ZN + NOX + DIS\" , data = data).fit()\n",
    "\n",
    "print(\"5FCV = \", -1*cross_val_score(lr, X_all, y, cv=5, scoring=\"neg_mean_squared_error\").mean())\n",
    "print(\"Adjusted R^2 = \", lm_all.rsquared_adj)\n",
    "print(\"AIC = \", lm_all.aic)\n",
    "print(\"BIC = \", lm_all.bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these comparisons, there is not really a conclusively best model. But I would prefer Model 2, which was generated through backward selection. The everything model scored well on adjusted $R^2$, but fared most poorly on 5FCV, AIC and BIC. Model 2 scored better than Model 1 on 5FCV, adjusted $R^2$ and AIC, but a little worse on BIC. (note: at the last minute, I got my Best Subset algorithm to work, which identified model 2 as the \"best\" overall subset. This gives additional rationale to use model 2, as the algorithm compared the model exhaustively to every other possible model)\n",
    "\n",
    "In comparison to the mathematical adjustment approaches, the 5FCV method gave roughly similar comparisons of model performance. However, 5FCV provides a more direct estimate of the test error. So while both approaches help us avoid the pitfall of overfitting the training set with too many predictors, and while the estimates tend to agree (at least in this case), the 5FCV approach is a more direct way to estimate model performance on the test subset than the mathematical adjustment approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Does your chosen model involve all of the features in the data\n",
    "set? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model does not involve all of the features. The reason for this is that the criteria I used to determine the best model impose penalties for adding additional variables to the model. So even though adding additional predictors always weakly increases $R^2$, it does not necessarily increase the criteria we have applied. Rather the models that perform best tend to be more limited to just the variables that have the most predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
